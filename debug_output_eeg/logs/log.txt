I20251209 12:51:13 29744 dinov2 config.py:59] git:
  sha: 22f724b10da1b8aa87415310783c7b02921fa4c1, status: has uncommitted changes, branch: main

I20251209 12:51:13 29744 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 12:51:13 29744 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 5e-05
I20251209 12:51:13 29744 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 16
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 14
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: /home_data/home/luotao2024/v-luotao/projects/pretrain/dinov2/dinov2/weights/dinov2_vitb14_reg4_pretrain.pth
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 5.0e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 518
  local_crops_size: 224
evaluation:
  eval_period_iterations: 1000

I20251209 12:51:24 29921 dinov2 config.py:59] git:
  sha: 22f724b10da1b8aa87415310783c7b02921fa4c1, status: has uncommitted changes, branch: main

I20251209 12:51:24 29921 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 12:51:24 29921 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 5e-05
I20251209 12:51:24 29921 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 16
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 14
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: /home_data/home/luotao2024/v-luotao/projects/pretrain/dinov2/dinov2/weights/dinov2_vitb14_reg4_pretrain.pth
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 5.0e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 518
  local_crops_size: 224
evaluation:
  eval_period_iterations: 1000

I20251209 12:52:18 30232 dinov2 config.py:59] git:
  sha: 22f724b10da1b8aa87415310783c7b02921fa4c1, status: has uncommitted changes, branch: main

I20251209 12:52:18 30232 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 12:52:18 30232 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 5e-05
I20251209 12:52:18 30232 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 16
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 14
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: /home_data/home/luotao2024/v-luotao/projects/pretrain/dinov2/dinov2/weights/dinov2_vitb14_reg4_pretrain.pth
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 5.0e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 518
  local_crops_size: 224
evaluation:
  eval_period_iterations: 1000

I20251209 13:24:37 32475 dinov2 config.py:59] git:
  sha: 22f724b10da1b8aa87415310783c7b02921fa4c1, status: has uncommitted changes, branch: main

I20251209 13:24:37 32475 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 13:24:37 32475 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 13:24:37 32475 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 14
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 518
  local_crops_size: 224
evaluation:
  eval_period_iterations: 1000

I20251209 13:24:49 32641 dinov2 config.py:59] git:
  sha: 22f724b10da1b8aa87415310783c7b02921fa4c1, status: has uncommitted changes, branch: main

I20251209 13:24:49 32641 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 13:24:49 32641 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 13:24:49 32641 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 14
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 518
  local_crops_size: 224
evaluation:
  eval_period_iterations: 1000

I20251209 13:26:13 32876 dinov2 config.py:59] git:
  sha: 22f724b10da1b8aa87415310783c7b02921fa4c1, status: has uncommitted changes, branch: main

I20251209 13:26:13 32876 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 13:26:13 32876 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 13:26:13 32876 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 14
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 518
  local_crops_size: 224
evaluation:
  eval_period_iterations: 1000

I20251209 13:29:18 33331 dinov2 config.py:59] git:
  sha: ecd87119e8cd63a5439b15eb8114da3d91ec941e, status: clean, branch: main

I20251209 13:29:18 33331 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 13:29:18 33331 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 13:29:18 33331 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 14
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 518
  local_crops_size: 224
evaluation:
  eval_period_iterations: 1000

I20251209 13:31:29 33625 dinov2 config.py:59] git:
  sha: ecd87119e8cd63a5439b15eb8114da3d91ec941e, status: has uncommitted changes, branch: main

I20251209 13:31:29 33625 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 13:31:29 33625 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 13:31:29 33625 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 14
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 518
  local_crops_size: 224
evaluation:
  eval_period_iterations: 1000

I20251209 13:47:06 34323 dinov2 config.py:59] git:
  sha: ecd87119e8cd63a5439b15eb8114da3d91ec941e, status: has uncommitted changes, branch: main

I20251209 13:47:06 34323 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 13:47:06 34323 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 13:47:06 34323 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
  num_channels: 19
  num_patches_per_channel: 30
  patch_time_dim: 250
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 1000

I20251209 13:47:06 34323 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:47:07 34323 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 13:47:07 34323 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 13:47:07 34323 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 13:47:08 34323 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 13:47:08 34323 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 13:47:08 34323 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 13:47:08 34323 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20251209 13:53:21 34988 dinov2 config.py:59] git:
  sha: ecd87119e8cd63a5439b15eb8114da3d91ec941e, status: has uncommitted changes, branch: main

I20251209 13:53:21 34988 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 13:53:21 34988 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 13:53:21 34988 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
  num_channels: 19
  num_patches_per_channel: 30
  patch_time_dim: 250
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 1000

I20251209 13:53:21 34988 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:53:22 34988 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 13:53:22 34988 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 13:53:22 34988 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 13:53:23 34988 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 13:53:23 34988 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 13:53:23 34988 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20251209 13:55:00 35314 dinov2 config.py:59] git:
  sha: ecd87119e8cd63a5439b15eb8114da3d91ec941e, status: has uncommitted changes, branch: main

I20251209 13:55:00 35314 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 13:55:00 35314 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 13:55:00 35314 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
  num_channels: 19
  num_patches_per_channel: 30
  patch_time_dim: 250
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 1000

I20251209 13:55:00 35314 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:55:00 35314 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 13:55:01 35314 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 13:55:01 35314 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 13:55:01 35314 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 13:55:01 35314 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20251209 13:56:47 35672 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 13:56:47 35672 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 13:56:47 35672 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
  num_channels: 19
  num_patches_per_channel: 30
  patch_time_dim: 250
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 1000

I20251209 13:56:48 35672 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:56:48 35672 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 13:56:49 35672 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 13:56:49 35672 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 13:56:49 35672 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 13:56:49 35672 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20251209 13:57:06 35909 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 13:57:06 35909 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 13:57:06 35909 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
  num_channels: 19
  num_patches_per_channel: 30
  patch_time_dim: 250
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 1000

I20251209 13:57:06 35909 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:57:07 35909 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 13:57:07 35909 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 13:57:07 35909 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 13:57:08 35909 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 13:57:08 35909 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 13:57:08 35909 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 13:57:08 35909 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20251209 13:59:08 36168 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 13:59:08 36168 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
  num_channels: 19
  num_patches_per_channel: 30
  patch_time_dim: 250
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 1000

I20251209 13:59:08 36168 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:59:09 36168 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:59:09 36168 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 13:59:09 36168 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 13:59:09 36168 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 13:59:09 36168 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 13:59:09 36168 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 13:59:09 36168 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 13:59:09 36168 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 13:59:10 36168 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 13:59:10 36168 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 13:59:10 36168 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:59:10 36168 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 13:59:10 36168 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 13:59:10 36168 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:59:10 36168 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 13:59:10 36168 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 13:59:10 36168 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 13:59:10 36168 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 13:59:10 36168 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 13:59:10 36168 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 13:59:10 36168 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20251209 13:59:35 36461 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 13:59:35 36461 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
  num_channels: 19
  num_patches_per_channel: 30
  patch_time_dim: 250
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 1000

I20251209 13:59:35 36461 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:59:36 36461 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 13:59:36 36461 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 13:59:36 36461 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 13:59:37 36461 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 13:59:37 36461 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 13:59:37 36461 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 13:59:37 36461 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20251209 14:02:07 36837 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 14:02:07 36837 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 14:02:07 36837 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
dino:
  loss_weight: 1.0
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: true
  head_n_prototypes: 65536
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 2
  dataset_path: ImageNet:split=TRAIN
  output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
  saveckp_freq: 20
  seed: 0
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: sinkhorn_knopp
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 0
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 4
  interpolate_antialias: false
  interpolate_offset: 0.1
  num_channels: 19
  num_patches_per_channel: 30
  patch_time_dim: 250
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 60
optim:
  epochs: 200
  weight_decay: 0.04
  weight_decay_end: 0.2
  base_lr: 0.0004
  lr: 1.767766952966369e-05
  warmup_epochs: 60
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 1.0
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 1000

I20251209 14:02:07 36837 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 14:02:07 36837 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 14:02:08 36837 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 14:02:08 36837 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 14:02:08 36837 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 14:02:09 36837 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20251209 14:03:15 37056 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 14:03:15 37056 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 14:03:15 37056 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 14:03:15 37056 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 14:03:16 37056 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 14:03:16 37056 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 14:03:16 37056 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 14:03:17 37056 dinov2 train.py:304] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): DropPath()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): DropPath()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Linear(in_features=250, out_features=768, bias=True)
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0-11): 12 x NestedTensorBlock(
            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (attn): MemEffAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): LayerScale()
            (drop_path1): Identity()
            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ls2): LayerScale()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
    (ibot_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=65536, bias=False)
      )
    )
  )
)
I20251209 14:18:29 37995 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 14:18:29 37995 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 14:18:29 37995 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 14:18:30 37995 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 14:18:30 37995 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 14:18:30 37995 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 14:18:30 37995 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 14:18:30 37995 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 14:18:30 37995 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 14:18:30 37995 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 14:18:30 37995 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 14:18:31 37995 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 14:18:31 37995 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 14:18:31 37995 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 14:18:31 37995 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 14:18:31 37995 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 14:18:31 37995 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 14:18:31 37995 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 14:18:31 37995 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 14:18:31 37995 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 14:18:31 37995 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 14:18:31 37995 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 14:18:31 37995 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 14:22:54 37995 dinov2 param_groups.py:54] chunked fsdp
I20251209 14:22:54 37995 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 14:22:54 37995 dinov2 param_groups.py:64] else code branch
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 14:22:54 37995 dinov2 param_groups.py:64] else code branch
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:22:54 37995 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 14:22:54 37995 dinov2 train.py:106] Schedulers ready.
W20251209 14:22:54 37995 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 14:22:54 37995 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 14:30:14 38743 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 14:30:14 38743 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 14:30:14 38743 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 14:30:15 38743 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 14:30:15 38743 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 14:30:15 38743 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 14:30:16 38743 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 14:30:16 38743 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 14:30:20 38743 dinov2 param_groups.py:54] chunked fsdp
I20251209 14:30:20 38743 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 14:30:20 38743 dinov2 param_groups.py:64] else code branch
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 14:30:20 38743 dinov2 param_groups.py:64] else code branch
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:30:20 38743 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 14:30:20 38743 dinov2 train.py:106] Schedulers ready.
W20251209 14:30:20 38743 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 14:30:20 38743 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 14:30:59 39000 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 14:30:59 39000 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 14:30:59 39000 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 14:30:59 39000 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 14:31:00 39000 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 14:31:00 39000 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 14:31:00 39000 dinov2 param_groups.py:54] chunked fsdp
I20251209 14:31:00 39000 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 14:31:00 39000 dinov2 param_groups.py:64] else code branch
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 14:31:00 39000 dinov2 param_groups.py:64] else code branch
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 14:31:00 39000 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 14:31:00 39000 dinov2 train.py:106] Schedulers ready.
W20251209 14:31:00 39000 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 14:31:00 39000 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 16:42:42 44271 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 16:42:42 44271 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 16:42:42 44271 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 16:42:43 44271 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 16:42:43 44271 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 16:42:43 44271 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 16:42:44 44271 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 16:42:44 44271 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 16:42:44 44271 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 16:42:44 44271 dinov2 param_groups.py:54] chunked fsdp
I20251209 16:42:44 44271 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 16:42:44 44271 dinov2 param_groups.py:64] else code branch
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 16:42:44 44271 dinov2 param_groups.py:64] else code branch
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:42:44 44271 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 16:42:44 44271 dinov2 train.py:106] Schedulers ready.
W20251209 16:42:44 44271 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 16:42:44 44271 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 16:44:15 44603 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 16:44:15 44603 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 16:44:15 44603 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 16:44:15 44603 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 16:44:16 44603 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 16:44:16 44603 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 16:44:16 44603 dinov2 param_groups.py:54] chunked fsdp
I20251209 16:44:16 44603 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 16:44:16 44603 dinov2 param_groups.py:64] else code branch
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 16:44:16 44603 dinov2 param_groups.py:64] else code branch
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 16:44:16 44603 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 16:44:16 44603 dinov2 train.py:106] Schedulers ready.
W20251209 16:44:16 44603 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 16:44:16 44603 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 16:44:16 44603 dinov2 loaders.py:35] Making EEG dataset from: "./samples"
I20251209 16:44:16 44603 dinov2 loaders.py:48] # of dataset samples: 2
I20251209 18:42:19 47169 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 18:42:19 47169 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 18:42:19 47169 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 18:42:20 47169 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 18:42:20 47169 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 18:42:20 47169 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 18:42:21 47169 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 18:42:21 47169 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 18:42:21 47169 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 18:42:21 47169 dinov2 param_groups.py:54] chunked fsdp
I20251209 18:42:21 47169 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 18:42:21 47169 dinov2 param_groups.py:64] else code branch
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 18:42:21 47169 dinov2 param_groups.py:64] else code branch
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:42:21 47169 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 18:42:21 47169 dinov2 train.py:106] Schedulers ready.
W20251209 18:42:21 47169 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 18:42:21 47169 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 18:42:21 47169 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 18:42:21 47169 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 18:42:21 47169 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 18:42:21 47169 dinov2 loaders.py:141] using PyTorch data loader
I20251209 18:42:21 47169 dinov2 loaders.py:156] infinite data loader
I20251209 18:46:15 47444 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 18:46:15 47444 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 18:46:15 47444 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 18:46:16 47444 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 18:46:16 47444 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 18:46:16 47444 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 18:46:17 47444 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 18:46:17 47444 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 18:46:17 47444 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 18:46:17 47444 dinov2 param_groups.py:54] chunked fsdp
I20251209 18:46:17 47444 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 18:46:17 47444 dinov2 param_groups.py:64] else code branch
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 18:46:17 47444 dinov2 param_groups.py:64] else code branch
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:46:17 47444 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 18:46:17 47444 dinov2 train.py:106] Schedulers ready.
W20251209 18:46:17 47444 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 18:46:17 47444 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 18:46:17 47444 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 18:46:17 47444 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 18:46:17 47444 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 18:46:17 47444 dinov2 loaders.py:141] using PyTorch data loader
I20251209 18:46:17 47444 dinov2 loaders.py:156] infinite data loader
I20251209 18:46:17 47444 dinov2 train.py:223] Starting training from iteration 0
I20251209 18:53:00 48803 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 18:53:00 48803 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 18:53:00 48803 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 18:53:01 48803 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 18:53:01 48803 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 18:53:01 48803 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 18:53:02 48803 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 18:53:02 48803 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 18:53:02 48803 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 18:53:02 48803 dinov2 param_groups.py:54] chunked fsdp
I20251209 18:53:02 48803 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 18:53:02 48803 dinov2 param_groups.py:64] else code branch
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 18:53:02 48803 dinov2 param_groups.py:64] else code branch
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 18:53:02 48803 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 18:53:02 48803 dinov2 train.py:106] Schedulers ready.
W20251209 18:53:02 48803 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 18:53:02 48803 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 18:53:02 48803 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 18:53:02 48803 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 18:53:02 48803 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 18:53:02 48803 dinov2 loaders.py:141] using PyTorch data loader
I20251209 18:53:02 48803 dinov2 loaders.py:156] infinite data loader
I20251209 18:53:02 48803 dinov2 train.py:223] Starting training from iteration 0
I20251209 19:22:05 51326 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 19:22:05 51326 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 19:22:05 51326 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 19:22:06 51326 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 19:22:06 51326 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 19:22:06 51326 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 19:22:07 51326 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 19:22:07 51326 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 19:22:07 51326 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 19:22:07 51326 dinov2 param_groups.py:54] chunked fsdp
I20251209 19:22:07 51326 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:22:07 51326 dinov2 param_groups.py:64] else code branch
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:22:07 51326 dinov2 param_groups.py:64] else code branch
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:22:07 51326 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:22:07 51326 dinov2 train.py:106] Schedulers ready.
W20251209 19:22:07 51326 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 19:22:07 51326 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 19:22:07 51326 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 19:22:07 51326 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 19:22:07 51326 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 19:22:07 51326 dinov2 loaders.py:141] using PyTorch data loader
I20251209 19:22:07 51326 dinov2 loaders.py:156] infinite data loader
I20251209 19:22:07 51326 dinov2 train.py:223] Starting training from iteration 0
I20251209 19:53:08 52865 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 19:53:08 52865 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 19:53:08 52865 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 19:53:08 52865 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 19:53:09 52865 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 19:53:09 52865 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 19:53:09 52865 dinov2 param_groups.py:54] chunked fsdp
I20251209 19:53:09 52865 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:53:09 52865 dinov2 param_groups.py:64] else code branch
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:53:09 52865 dinov2 param_groups.py:64] else code branch
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:09 52865 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:53:09 52865 dinov2 train.py:106] Schedulers ready.
W20251209 19:53:09 52865 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 19:53:09 52865 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 19:53:09 52865 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 19:53:09 52865 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 19:53:09 52865 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 19:53:09 52865 dinov2 loaders.py:141] using PyTorch data loader
I20251209 19:53:09 52865 dinov2 loaders.py:156] infinite data loader
I20251209 19:53:09 52865 dinov2 train.py:223] Starting training from iteration 0
I20251209 19:53:38 53728 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 19:53:38 53728 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 19:53:38 53728 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 19:53:38 53728 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 19:53:39 53728 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 19:53:39 53728 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 19:53:39 53728 dinov2 param_groups.py:54] chunked fsdp
I20251209 19:53:39 53728 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:53:39 53728 dinov2 param_groups.py:64] else code branch
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:53:39 53728 dinov2 param_groups.py:64] else code branch
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:53:39 53728 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:53:39 53728 dinov2 train.py:106] Schedulers ready.
W20251209 19:53:39 53728 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 19:53:39 53728 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 19:53:39 53728 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 19:53:39 53728 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 19:53:39 53728 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 19:53:39 53728 dinov2 loaders.py:141] using PyTorch data loader
I20251209 19:53:39 53728 dinov2 loaders.py:156] infinite data loader
I20251209 19:53:39 53728 dinov2 train.py:223] Starting training from iteration 0
I20251209 19:53:59 54474 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 19:53:59 54474 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 19:53:59 54474 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 19:54:00 54474 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 19:54:00 54474 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 19:54:00 54474 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 19:54:00 54474 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 19:54:00 54474 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 19:54:00 54474 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 19:54:00 54474 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 19:54:00 54474 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 19:54:01 54474 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 19:54:01 54474 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 19:54:01 54474 dinov2 param_groups.py:54] chunked fsdp
I20251209 19:54:01 54474 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:54:01 54474 dinov2 param_groups.py:64] else code branch
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:54:01 54474 dinov2 param_groups.py:64] else code branch
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:54:01 54474 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:54:01 54474 dinov2 train.py:106] Schedulers ready.
W20251209 19:54:01 54474 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 19:54:01 54474 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 19:54:01 54474 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 19:54:01 54474 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 19:54:01 54474 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 19:54:01 54474 dinov2 loaders.py:141] using PyTorch data loader
I20251209 19:54:01 54474 dinov2 loaders.py:156] infinite data loader
I20251209 19:54:01 54474 dinov2 train.py:223] Starting training from iteration 0
I20251209 19:56:05 55418 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 19:56:05 55418 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 19:56:05 55418 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 19:56:05 55418 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 19:56:06 55418 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 19:56:06 55418 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 19:56:06 55418 dinov2 param_groups.py:54] chunked fsdp
I20251209 19:56:06 55418 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:56:06 55418 dinov2 param_groups.py:64] else code branch
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:56:06 55418 dinov2 param_groups.py:64] else code branch
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 19:56:06 55418 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 19:56:06 55418 dinov2 train.py:106] Schedulers ready.
W20251209 19:56:06 55418 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 19:56:06 55418 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 19:56:06 55418 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 19:56:06 55418 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 19:56:06 55418 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 19:56:06 55418 dinov2 loaders.py:141] using PyTorch data loader
I20251209 19:56:06 55418 dinov2 loaders.py:156] infinite data loader
I20251209 19:56:06 55418 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:03:34 56792 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:03:34 56792 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:03:34 56792 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:03:35 56792 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:03:35 56792 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:03:35 56792 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:03:36 56792 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:03:36 56792 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:03:36 56792 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:03:36 56792 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:03:36 56792 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:03:36 56792 dinov2 param_groups.py:64] else code branch
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:03:36 56792 dinov2 param_groups.py:64] else code branch
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:03:36 56792 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:03:36 56792 dinov2 train.py:106] Schedulers ready.
W20251209 20:03:36 56792 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:03:36 56792 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:03:36 56792 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:03:36 56792 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:03:36 56792 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:03:36 56792 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:03:36 56792 dinov2 loaders.py:156] infinite data loader
I20251209 20:03:36 56792 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:09:21 57763 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:09:21 57763 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:09:21 57763 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:09:22 57763 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:09:22 57763 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:09:22 57763 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:09:23 57763 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:09:23 57763 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:09:23 57763 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:09:23 57763 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:09:23 57763 dinov2 param_groups.py:64] else code branch
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:09:23 57763 dinov2 param_groups.py:64] else code branch
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:23 57763 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:09:23 57763 dinov2 train.py:106] Schedulers ready.
W20251209 20:09:23 57763 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:09:23 57763 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:09:23 57763 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:09:23 57763 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:09:23 57763 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:09:23 57763 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:09:23 57763 dinov2 loaders.py:156] infinite data loader
I20251209 20:09:23 57763 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:09:46 58460 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:09:46 58460 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:09:46 58460 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:09:46 58460 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:09:47 58460 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:09:47 58460 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:09:48 58460 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:09:48 58460 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:09:48 58460 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:09:48 58460 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:09:48 58460 dinov2 param_groups.py:64] else code branch
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:09:48 58460 dinov2 param_groups.py:64] else code branch
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:09:48 58460 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:09:48 58460 dinov2 train.py:106] Schedulers ready.
W20251209 20:09:48 58460 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:09:48 58460 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:09:48 58460 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:09:48 58460 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:09:48 58460 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:09:48 58460 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:09:48 58460 dinov2 loaders.py:156] infinite data loader
I20251209 20:09:48 58460 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:16:05 59547 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:16:05 59547 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:16:05 59547 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:16:06 59547 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:16:06 59547 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:16:06 59547 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:16:06 59547 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:16:06 59547 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:16:06 59547 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:16:06 59547 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:16:06 59547 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:16:06 59547 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:16:07 59547 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:16:07 59547 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:16:07 59547 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:16:07 59547 dinov2 param_groups.py:64] else code branch
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:16:07 59547 dinov2 param_groups.py:64] else code branch
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:16:07 59547 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:16:07 59547 dinov2 train.py:106] Schedulers ready.
W20251209 20:16:07 59547 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:16:07 59547 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:16:07 59547 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:16:07 59547 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:16:07 59547 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:16:07 59547 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:16:07 59547 dinov2 loaders.py:156] infinite data loader
I20251209 20:16:07 59547 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:25:45 60475 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:25:45 60475 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:25:45 60475 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:25:45 60475 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:25:46 60475 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:25:46 60475 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:25:46 60475 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:25:46 60475 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:25:46 60475 dinov2 param_groups.py:64] else code branch
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:25:46 60475 dinov2 param_groups.py:64] else code branch
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:25:46 60475 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:25:46 60475 dinov2 train.py:106] Schedulers ready.
W20251209 20:25:46 60475 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:25:46 60475 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:25:46 60475 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:25:46 60475 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:25:46 60475 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:25:46 60475 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:25:46 60475 dinov2 loaders.py:156] infinite data loader
I20251209 20:25:46 60475 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:26:09 61191 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:26:09 61191 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:26:09 61191 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:26:09 61191 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:26:10 61191 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:26:10 61191 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:26:11 61191 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:26:11 61191 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:26:11 61191 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:26:11 61191 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:26:11 61191 dinov2 param_groups.py:64] else code branch
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:26:11 61191 dinov2 param_groups.py:64] else code branch
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:26:11 61191 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:26:11 61191 dinov2 train.py:106] Schedulers ready.
W20251209 20:26:11 61191 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:26:11 61191 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:26:11 61191 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:26:11 61191 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:26:11 61191 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:26:11 61191 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:26:11 61191 dinov2 loaders.py:156] infinite data loader
I20251209 20:26:11 61191 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:29:17 61830 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:29:17 61830 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:29:17 61830 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:29:18 61830 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:29:18 61830 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:29:18 61830 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:29:19 61830 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:29:19 61830 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:29:19 61830 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:29:19 61830 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:29:19 61830 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:29:19 61830 dinov2 param_groups.py:64] else code branch
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:29:19 61830 dinov2 param_groups.py:64] else code branch
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:29:19 61830 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:29:19 61830 dinov2 train.py:106] Schedulers ready.
W20251209 20:29:19 61830 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:29:19 61830 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:29:19 61830 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:29:19 61830 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:29:19 61830 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:29:19 61830 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:29:19 61830 dinov2 loaders.py:156] infinite data loader
I20251209 20:29:19 61830 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:31:50 62590 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:31:50 62590 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:31:50 62590 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:31:50 62590 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:31:51 62590 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:31:51 62590 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:31:51 62590 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:31:52 62590 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:31:52 62590 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:31:52 62590 dinov2 param_groups.py:64] else code branch
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:31:52 62590 dinov2 param_groups.py:64] else code branch
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:31:52 62590 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:31:52 62590 dinov2 train.py:106] Schedulers ready.
W20251209 20:31:52 62590 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:31:52 62590 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:31:52 62590 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:31:52 62590 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:31:52 62590 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:31:52 62590 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:31:52 62590 dinov2 loaders.py:156] infinite data loader
I20251209 20:31:52 62590 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:47:14 64124 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:47:14 64124 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:47:14 64124 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:47:15 64124 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:47:15 64124 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:47:15 64124 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:47:16 64124 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:47:16 64124 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:47:16 64124 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:47:16 64124 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:47:16 64124 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:47:16 64124 dinov2 param_groups.py:64] else code branch
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:47:16 64124 dinov2 param_groups.py:64] else code branch
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:47:16 64124 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:47:16 64124 dinov2 train.py:106] Schedulers ready.
W20251209 20:47:16 64124 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:47:16 64124 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:47:16 64124 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:47:16 64124 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:47:16 64124 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:47:16 64124 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:47:16 64124 dinov2 loaders.py:156] infinite data loader
I20251209 20:47:16 64124 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:50:08 65005 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:50:08 65005 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:50:08 65005 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:50:09 65005 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:50:09 65005 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:50:09 65005 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:50:09 65005 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:50:09 65005 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:50:09 65005 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:50:09 65005 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:50:09 65005 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:50:09 65005 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:50:10 65005 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:50:10 65005 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:50:10 65005 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:50:10 65005 dinov2 param_groups.py:64] else code branch
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:50:10 65005 dinov2 param_groups.py:64] else code branch
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:10 65005 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:50:10 65005 dinov2 train.py:106] Schedulers ready.
W20251209 20:50:10 65005 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:50:10 65005 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:50:10 65005 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:50:10 65005 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:50:10 65005 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:50:10 65005 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:50:10 65005 dinov2 loaders.py:156] infinite data loader
I20251209 20:50:10 65005 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:50:35 65676 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:50:35 65676 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:50:35 65676 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:50:36 65676 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:50:36 65676 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:50:36 65676 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:50:36 65676 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:50:36 65676 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:50:36 65676 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:50:36 65676 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:50:36 65676 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:50:36 65676 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:50:37 65676 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:50:37 65676 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:50:37 65676 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:50:37 65676 dinov2 param_groups.py:64] else code branch
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:50:37 65676 dinov2 param_groups.py:64] else code branch
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:50:37 65676 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:50:37 65676 dinov2 train.py:106] Schedulers ready.
W20251209 20:50:37 65676 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:50:37 65676 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:50:37 65676 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:50:37 65676 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:50:37 65676 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:50:37 65676 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:50:37 65676 dinov2 loaders.py:156] infinite data loader
I20251209 20:50:37 65676 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:52:01 66498 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:52:01 66498 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:52:01 66498 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:52:02 66498 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:52:02 66498 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:52:02 66498 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:52:02 66498 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:52:02 66498 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:52:02 66498 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:52:02 66498 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:52:02 66498 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:52:02 66498 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:52:03 66498 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:52:03 66498 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:52:03 66498 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:52:03 66498 dinov2 param_groups.py:64] else code branch
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:52:03 66498 dinov2 param_groups.py:64] else code branch
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:52:03 66498 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:52:03 66498 dinov2 train.py:106] Schedulers ready.
W20251209 20:52:03 66498 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:52:03 66498 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:52:03 66498 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:52:03 66498 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:52:03 66498 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:52:03 66498 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:52:03 66498 dinov2 loaders.py:156] infinite data loader
I20251209 20:52:03 66498 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:53:42 67228 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:53:42 67228 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:53:42 67228 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:53:43 67228 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:53:43 67228 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:53:43 67228 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:53:44 67228 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:53:44 67228 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:53:44 67228 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:53:44 67228 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:53:44 67228 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:53:44 67228 dinov2 param_groups.py:64] else code branch
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:53:44 67228 dinov2 param_groups.py:64] else code branch
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:53:44 67228 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:53:44 67228 dinov2 train.py:106] Schedulers ready.
W20251209 20:53:44 67228 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:53:44 67228 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:53:44 67228 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:53:44 67228 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:53:44 67228 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:53:44 67228 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:53:44 67228 dinov2 loaders.py:156] infinite data loader
I20251209 20:53:44 67228 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:55:04 67965 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:55:04 67965 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:55:04 67965 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:55:04 67965 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:55:05 67965 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:55:05 67965 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:55:05 67965 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:55:05 67965 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:55:05 67965 dinov2 param_groups.py:64] else code branch
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:55:05 67965 dinov2 param_groups.py:64] else code branch
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:05 67965 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:55:05 67965 dinov2 train.py:106] Schedulers ready.
W20251209 20:55:05 67965 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:55:05 67965 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:55:05 67965 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:55:05 67965 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:55:05 67965 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:55:05 67965 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:55:05 67965 dinov2 loaders.py:156] infinite data loader
I20251209 20:55:05 67965 dinov2 train.py:223] Starting training from iteration 0
I20251209 20:55:45 68793 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 20:55:45 68793 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 20:55:45 68793 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:55:46 68793 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 20:55:47 68793 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 20:55:47 68793 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 20:55:47 68793 dinov2 param_groups.py:54] chunked fsdp
I20251209 20:55:47 68793 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:55:47 68793 dinov2 param_groups.py:64] else code branch
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:55:47 68793 dinov2 param_groups.py:64] else code branch
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 20:55:47 68793 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 20:55:47 68793 dinov2 train.py:106] Schedulers ready.
W20251209 20:55:47 68793 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 20:55:47 68793 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 20:55:47 68793 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 20:55:47 68793 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 20:55:47 68793 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 20:55:47 68793 dinov2 loaders.py:141] using PyTorch data loader
I20251209 20:55:47 68793 dinov2 loaders.py:156] infinite data loader
I20251209 20:55:47 68793 dinov2 train.py:223] Starting training from iteration 0
I20251209 21:03:52 69868 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 21:03:52 69868 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 21:03:52 69868 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:03:53 69868 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:03:53 69868 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 21:03:53 69868 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 21:03:53 69868 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 21:03:53 69868 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 21:03:53 69868 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 21:03:53 69868 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 21:03:53 69868 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 21:03:54 69868 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 21:03:54 69868 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 21:03:54 69868 dinov2 param_groups.py:54] chunked fsdp
I20251209 21:03:54 69868 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 21:03:54 69868 dinov2 param_groups.py:64] else code branch
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 21:03:54 69868 dinov2 param_groups.py:64] else code branch
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:03:54 69868 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 21:03:54 69868 dinov2 train.py:106] Schedulers ready.
W20251209 21:03:54 69868 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 21:03:54 69868 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 21:03:54 69868 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 21:03:54 69868 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 21:03:54 69868 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 21:03:54 69868 dinov2 loaders.py:141] using PyTorch data loader
I20251209 21:03:54 69868 dinov2 loaders.py:156] infinite data loader
I20251209 21:03:54 69868 dinov2 train.py:223] Starting training from iteration 0
W20251209 21:04:19 69868 py.warnings warnings.py:112] /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/dinov2/loss/koleo_loss.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):

I20251209 21:05:43 70737 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 21:05:43 70737 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 21:05:43 70737 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:05:44 70737 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 21:05:44 70737 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 21:05:44 70737 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 21:05:45 70737 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 21:05:45 70737 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 21:05:45 70737 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 21:05:45 70737 dinov2 param_groups.py:54] chunked fsdp
I20251209 21:05:45 70737 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 21:05:45 70737 dinov2 param_groups.py:64] else code branch
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 21:05:45 70737 dinov2 param_groups.py:64] else code branch
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:05:45 70737 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 21:05:45 70737 dinov2 train.py:106] Schedulers ready.
W20251209 21:05:45 70737 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 21:05:45 70737 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 21:05:45 70737 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 21:05:45 70737 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 21:05:45 70737 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 21:05:45 70737 dinov2 loaders.py:141] using PyTorch data loader
I20251209 21:05:45 70737 dinov2 loaders.py:156] infinite data loader
I20251209 21:05:45 70737 dinov2 train.py:223] Starting training from iteration 0
W20251209 21:05:46 70737 py.warnings warnings.py:112] /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/dinov2/loss/koleo_loss.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):

I20251209 21:09:42 71645 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 21:09:42 71645 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 21:09:42 71645 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:09:43 71645 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 21:09:43 71645 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 21:09:43 71645 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 21:09:44 71645 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 21:09:44 71645 dinov2 ssl_meta_arch.py:417] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 21:09:44 71645 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 21:09:44 71645 dinov2 param_groups.py:54] chunked fsdp
I20251209 21:09:44 71645 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 21:09:44 71645 dinov2 param_groups.py:64] else code branch
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 21:09:44 71645 dinov2 param_groups.py:64] else code branch
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:09:44 71645 dinov2 ssl_meta_arch.py:404] fusing param groups
I20251209 21:09:44 71645 dinov2 train.py:106] Schedulers ready.
W20251209 21:09:44 71645 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 21:09:44 71645 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 21:09:44 71645 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 21:09:44 71645 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 21:09:44 71645 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 21:09:44 71645 dinov2 loaders.py:141] using PyTorch data loader
I20251209 21:09:44 71645 dinov2 loaders.py:156] infinite data loader
I20251209 21:09:44 71645 dinov2 train.py:223] Starting training from iteration 0
W20251209 21:09:45 71645 py.warnings warnings.py:112] /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/dinov2/loss/koleo_loss.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):

I20251209 21:10:29 71645 dinov2 helpers.py:102] Training  [     0/250000]  eta: 131 days, 12:43:10  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7796 (14.7796)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6860 (0.6860)  ibot_loss: 2.8096 (2.8096)  time: 45.456760  data: 0.454201  max mem: 3122
W20251209 21:10:29 71645 py.warnings warnings.py:112] /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/dinov2/loss/koleo_loss.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):

W20251209 21:10:32 71645 py.warnings warnings.py:112] /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/dinov2/loss/koleo_loss.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):

I20251209 21:10:34 71645 dinov2 helpers.py:102] Training  [    10/250000]  eta: 13 days, 1:29:19  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7754 (14.7772)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6831 (0.6843)  ibot_loss: 2.8090 (2.8089)  time: 4.514417  data: 0.041599  max mem: 4132
I20251209 21:10:36 71645 dinov2 helpers.py:102] Training  [    20/250000]  eta: 7 days, 3:16:34  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7726 (14.7770)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6802 (0.6839)  ibot_loss: 2.8090 (2.8090)  time: 0.317068  data: 0.000315  max mem: 4135
I20251209 21:10:38 71645 dinov2 helpers.py:102] Training  [    30/250000]  eta: 5 days, 0:56:04  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7760 (14.7774)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6845)  ibot_loss: 2.8089 (2.8089)  time: 0.216654  data: 0.000336  max mem: 4143
I20251209 21:10:40 71645 dinov2 helpers.py:102] Training  [    40/250000]  eta: 3 days, 22:21:07  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7768 (14.7773)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6843)  ibot_loss: 2.8088 (2.8089)  time: 0.195817  data: 0.000352  max mem: 4143
I20251209 21:10:41 71645 dinov2 helpers.py:102] Training  [    50/250000]  eta: 3 days, 6:17:03  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7758 (14.7776)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6847)  ibot_loss: 2.8091 (2.8090)  time: 0.175589  data: 0.000327  max mem: 4143
I20251209 21:10:43 71645 dinov2 helpers.py:102] Training  [    60/250000]  eta: 2 days, 19:30:13  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7749 (14.7775)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6845)  ibot_loss: 2.8090 (2.8089)  time: 0.179753  data: 0.000344  max mem: 4143
I20251209 21:10:45 71645 dinov2 helpers.py:102] Training  [    70/250000]  eta: 2 days, 11:41:54  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7749 (14.7776)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6847)  ibot_loss: 2.8088 (2.8089)  time: 0.177468  data: 0.000349  max mem: 4148
I20251209 21:10:47 71645 dinov2 helpers.py:102] Training  [    80/250000]  eta: 2 days, 5:43:38  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7748 (14.7774)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6821 (0.6845)  ibot_loss: 2.8094 (2.8090)  time: 0.168906  data: 0.000296  max mem: 4148
I20251209 21:10:48 71645 dinov2 helpers.py:102] Training  [    90/250000]  eta: 2 days, 1:03:24  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7723 (14.7773)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6792 (0.6843)  ibot_loss: 2.8093 (2.8090)  time: 0.162727  data: 0.000235  max mem: 4148
I20251209 21:10:50 71645 dinov2 helpers.py:102] Training  [   100/250000]  eta: 1 day, 21:18:55  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7706 (14.7770)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6772 (0.6840)  ibot_loss: 2.8090 (2.8090)  time: 0.162275  data: 0.000241  max mem: 4148
I20251209 21:10:51 71645 dinov2 helpers.py:102] Training  [   110/250000]  eta: 1 day, 18:14:50  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7749 (14.7769)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6839)  ibot_loss: 2.8087 (2.8090)  time: 0.162538  data: 0.000243  max mem: 4148
I20251209 21:10:53 71645 dinov2 helpers.py:102] Training  [   120/250000]  eta: 1 day, 15:40:54  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7749 (14.7767)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6838)  ibot_loss: 2.8089 (2.8090)  time: 0.162083  data: 0.000218  max mem: 4148
I20251209 21:10:55 71645 dinov2 helpers.py:102] Training  [   130/250000]  eta: 1 day, 13:30:30  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7723 (14.7768)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6787 (0.6839)  ibot_loss: 2.8087 (2.8089)  time: 0.161730  data: 0.000234  max mem: 4148
I20251209 21:10:56 71645 dinov2 helpers.py:102] Training  [   140/250000]  eta: 1 day, 11:38:29  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7763 (14.7770)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6841)  ibot_loss: 2.8087 (2.8089)  time: 0.161608  data: 0.000256  max mem: 4148
I20251209 21:10:58 71645 dinov2 helpers.py:102] Training  [   150/250000]  eta: 1 day, 10:01:42  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7765 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6841)  ibot_loss: 2.8093 (2.8090)  time: 0.162162  data: 0.000239  max mem: 4148
I20251209 21:11:00 71645 dinov2 helpers.py:102] Training  [   160/250000]  eta: 1 day, 8:36:54  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7747 (14.7768)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6807 (0.6839)  ibot_loss: 2.8090 (2.8089)  time: 0.162815  data: 0.000222  max mem: 4148
I20251209 21:11:01 71645 dinov2 helpers.py:102] Training  [   170/250000]  eta: 1 day, 7:23:47  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7747 (14.7769)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6816 (0.6840)  ibot_loss: 2.8089 (2.8089)  time: 0.166387  data: 0.000249  max mem: 4149
I20251209 21:11:03 71645 dinov2 helpers.py:102] Training  [   180/250000]  eta: 1 day, 6:17:41  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7787 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6855 (0.6841)  ibot_loss: 2.8091 (2.8090)  time: 0.167723  data: 0.000260  max mem: 4149
I20251209 21:11:05 71645 dinov2 helpers.py:102] Training  [   190/250000]  eta: 1 day, 5:18:41  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7730 (14.7769)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6812 (0.6840)  ibot_loss: 2.8091 (2.8090)  time: 0.165792  data: 0.000262  max mem: 4149
I20251209 21:11:06 71645 dinov2 helpers.py:102] Training  [   200/250000]  eta: 1 day, 4:25:27  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7744 (14.7769)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6812 (0.6839)  ibot_loss: 2.8091 (2.8090)  time: 0.165961  data: 0.000269  max mem: 4149
I20251209 21:11:08 71645 dinov2 helpers.py:102] Training  [   210/250000]  eta: 1 day, 3:36:57  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7756 (14.7769)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6840)  ibot_loss: 2.8090 (2.8090)  time: 0.164967  data: 0.000264  max mem: 4149
I20251209 21:11:10 71645 dinov2 helpers.py:102] Training  [   220/250000]  eta: 1 day, 2:53:20  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7711 (14.7768)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6782 (0.6838)  ibot_loss: 2.8092 (2.8090)  time: 0.165508  data: 0.000294  max mem: 4149
I20251209 21:11:11 71645 dinov2 helpers.py:102] Training  [   230/250000]  eta: 1 day, 2:13:48  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7703 (14.7765)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6772 (0.6835)  ibot_loss: 2.8094 (2.8090)  time: 0.167669  data: 0.000316  max mem: 4149
I20251209 21:11:13 71645 dinov2 helpers.py:102] Training  [   240/250000]  eta: 1 day, 1:37:05  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7718 (14.7765)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6782 (0.6835)  ibot_loss: 2.8092 (2.8090)  time: 0.167164  data: 0.000286  max mem: 4149
I20251209 21:11:15 71645 dinov2 helpers.py:102] Training  [   250/250000]  eta: 1 day, 1:03:28  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7747 (14.7765)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6835)  ibot_loss: 2.8090 (2.8090)  time: 0.166371  data: 0.000262  max mem: 4149
I20251209 21:11:16 71645 dinov2 helpers.py:102] Training  [   260/250000]  eta: 1 day, 0:32:02  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7778 (14.7768)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6846 (0.6838)  ibot_loss: 2.8086 (2.8090)  time: 0.165703  data: 0.000272  max mem: 4149
I20251209 21:11:18 71645 dinov2 helpers.py:102] Training  [   270/250000]  eta: 1 day, 0:03:05  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7778 (14.7767)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6846 (0.6837)  ibot_loss: 2.8086 (2.8090)  time: 0.165049  data: 0.000283  max mem: 4149
I20251209 21:11:20 71645 dinov2 helpers.py:102] Training  [   280/250000]  eta: 23:37:22  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7803 (14.7769)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6875 (0.6839)  ibot_loss: 2.8087 (2.8090)  time: 0.169527  data: 0.000310  max mem: 4149
I20251209 21:11:21 71645 dinov2 helpers.py:102] Training  [   290/250000]  eta: 23:13:18  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7795 (14.7770)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6875 (0.6840)  ibot_loss: 2.8086 (2.8090)  time: 0.173060  data: 0.000326  max mem: 4149
I20251209 21:11:23 71645 dinov2 helpers.py:102] Training  [   300/250000]  eta: 22:49:52  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7767 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6841)  ibot_loss: 2.8091 (2.8090)  time: 0.169189  data: 0.000300  max mem: 4149
I20251209 21:11:25 71645 dinov2 helpers.py:102] Training  [   310/250000]  eta: 22:27:27  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7755 (14.7770)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6840)  ibot_loss: 2.8088 (2.8090)  time: 0.163836  data: 0.000275  max mem: 4149
I20251209 21:11:26 71645 dinov2 helpers.py:102] Training  [   320/250000]  eta: 22:06:26  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7759 (14.7770)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6821 (0.6841)  ibot_loss: 2.8088 (2.8090)  time: 0.162074  data: 0.000267  max mem: 4149
I20251209 21:11:28 71645 dinov2 helpers.py:102] Training  [   330/250000]  eta: 21:46:44  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7738 (14.7769)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6802 (0.6839)  ibot_loss: 2.8091 (2.8090)  time: 0.162310  data: 0.000271  max mem: 4149
I20251209 21:11:30 71645 dinov2 helpers.py:102] Training  [   340/250000]  eta: 21:28:17  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7741 (14.7769)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6812 (0.6839)  ibot_loss: 2.8090 (2.8090)  time: 0.162877  data: 0.000283  max mem: 4149
I20251209 21:11:31 71645 dinov2 helpers.py:102] Training  [   350/250000]  eta: 21:11:04  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7755 (14.7769)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6839)  ibot_loss: 2.8088 (2.8090)  time: 0.164045  data: 0.000283  max mem: 4149
I20251209 21:11:33 71645 dinov2 helpers.py:102] Training  [   360/250000]  eta: 20:55:07  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7754 (14.7769)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6839)  ibot_loss: 2.8084 (2.8090)  time: 0.166120  data: 0.000303  max mem: 4149
I20251209 21:11:35 71645 dinov2 helpers.py:102] Training  [   370/250000]  eta: 20:40:00  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7721 (14.7768)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6797 (0.6838)  ibot_loss: 2.8094 (2.8090)  time: 0.167444  data: 0.000328  max mem: 4149
I20251209 21:11:36 71645 dinov2 helpers.py:102] Training  [   380/250000]  eta: 20:25:31  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7722 (14.7767)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6787 (0.6838)  ibot_loss: 2.8090 (2.8090)  time: 0.166630  data: 0.000312  max mem: 4149
I20251209 21:11:38 71645 dinov2 helpers.py:102] Training  [   390/250000]  eta: 20:12:48  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7752 (14.7768)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6821 (0.6838)  ibot_loss: 2.8090 (2.8090)  time: 0.170686  data: 0.000330  max mem: 4149
I20251209 21:11:40 71645 dinov2 helpers.py:102] Training  [   400/250000]  eta: 20:00:49  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7736 (14.7767)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6812 (0.6838)  ibot_loss: 2.8087 (2.8090)  time: 0.175982  data: 0.000349  max mem: 4149
I20251209 21:11:41 71645 dinov2 helpers.py:102] Training  [   410/250000]  eta: 19:49:06  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7736 (14.7767)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6792 (0.6838)  ibot_loss: 2.8087 (2.8090)  time: 0.174913  data: 0.000354  max mem: 4149
I20251209 21:11:43 71645 dinov2 helpers.py:102] Training  [   420/250000]  eta: 19:37:34  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7803 (14.7768)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6875 (0.6839)  ibot_loss: 2.8092 (2.8090)  time: 0.171507  data: 0.000349  max mem: 4149
I20251209 21:11:45 71645 dinov2 helpers.py:102] Training  [   430/250000]  eta: 19:26:12  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7803 (14.7769)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6875 (0.6839)  ibot_loss: 2.8089 (2.8090)  time: 0.167661  data: 0.000308  max mem: 4149
I20251209 21:11:46 71645 dinov2 helpers.py:102] Training  [   440/250000]  eta: 19:15:20  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7769 (14.7769)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6846 (0.6839)  ibot_loss: 2.8090 (2.8090)  time: 0.165715  data: 0.000286  max mem: 4149
I20251209 21:11:48 71645 dinov2 helpers.py:102] Training  [   450/250000]  eta: 19:06:13  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7789 (14.7770)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6865 (0.6840)  ibot_loss: 2.8092 (2.8090)  time: 0.172599  data: 0.000284  max mem: 4149
I20251209 21:11:50 71645 dinov2 helpers.py:102] Training  [   460/250000]  eta: 18:57:01  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7808 (14.7770)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6875 (0.6840)  ibot_loss: 2.8093 (2.8090)  time: 0.176783  data: 0.000303  max mem: 4149
I20251209 21:11:52 71645 dinov2 helpers.py:102] Training  [   470/250000]  eta: 18:47:46  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7802 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6865 (0.6841)  ibot_loss: 2.8089 (2.8090)  time: 0.171599  data: 0.000334  max mem: 4149
I20251209 21:11:53 71645 dinov2 helpers.py:102] Training  [   480/250000]  eta: 18:39:11  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7768 (14.7772)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6842)  ibot_loss: 2.8089 (2.8090)  time: 0.170766  data: 0.000336  max mem: 4149
I20251209 21:11:55 71645 dinov2 helpers.py:102] Training  [   490/250000]  eta: 18:30:06  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7765 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6821 (0.6841)  ibot_loss: 2.8090 (2.8090)  time: 0.167378  data: 0.000310  max mem: 4149
I20251209 21:11:57 71645 dinov2 helpers.py:102] Training  [   500/250000]  eta: 18:21:32  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7766 (14.7772)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6846 (0.6842)  ibot_loss: 2.8090 (2.8090)  time: 0.163338  data: 0.000304  max mem: 4149
I20251209 21:11:58 71645 dinov2 helpers.py:102] Training  [   510/250000]  eta: 18:13:08  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7771 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6846 (0.6842)  ibot_loss: 2.8089 (2.8090)  time: 0.163281  data: 0.000299  max mem: 4149
I20251209 21:12:00 71645 dinov2 helpers.py:102] Training  [   520/250000]  eta: 18:05:37  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7768 (14.7772)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6846 (0.6843)  ibot_loss: 2.8089 (2.8090)  time: 0.165791  data: 0.000281  max mem: 4149
I20251209 21:12:02 71645 dinov2 helpers.py:102] Training  [   530/250000]  eta: 17:58:07  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7768 (14.7772)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6846 (0.6842)  ibot_loss: 2.8087 (2.8090)  time: 0.167514  data: 0.000269  max mem: 4149
I20251209 21:12:03 71645 dinov2 helpers.py:102] Training  [   540/250000]  eta: 17:50:49  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7752 (14.7772)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6831 (0.6842)  ibot_loss: 2.8087 (2.8090)  time: 0.165316  data: 0.000276  max mem: 4149
I20251209 21:12:05 71645 dinov2 helpers.py:102] Training  [   550/250000]  eta: 17:43:40  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7752 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6831 (0.6841)  ibot_loss: 2.8089 (2.8090)  time: 0.164173  data: 0.000270  max mem: 4149
I20251209 21:12:07 71645 dinov2 helpers.py:102] Training  [   560/250000]  eta: 17:36:48  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7753 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6821 (0.6841)  ibot_loss: 2.8089 (2.8090)  time: 0.163524  data: 0.000310  max mem: 4149
I20251209 21:12:08 71645 dinov2 helpers.py:102] Training  [   570/250000]  eta: 17:30:13  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7757 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6826 (0.6841)  ibot_loss: 2.8090 (2.8090)  time: 0.164104  data: 0.000336  max mem: 4149
I20251209 21:12:10 71645 dinov2 helpers.py:102] Training  [   580/250000]  eta: 17:23:45  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7755 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6831 (0.6841)  ibot_loss: 2.8089 (2.8090)  time: 0.163672  data: 0.000292  max mem: 4149
I20251209 21:12:11 71645 dinov2 helpers.py:102] Training  [   590/250000]  eta: 17:17:33  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7736 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6812 (0.6841)  ibot_loss: 2.8086 (2.8090)  time: 0.163162  data: 0.000277  max mem: 4149
I20251209 21:12:13 71645 dinov2 helpers.py:102] Training  [   600/250000]  eta: 17:11:37  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7770 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6836 (0.6841)  ibot_loss: 2.8088 (2.8090)  time: 0.163916  data: 0.000272  max mem: 4149
I20251209 21:12:15 71645 dinov2 helpers.py:102] Training  [   610/250000]  eta: 17:05:44  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7746 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6812 (0.6841)  ibot_loss: 2.8092 (2.8090)  time: 0.163293  data: 0.000270  max mem: 4149
I20251209 21:12:16 71645 dinov2 helpers.py:102] Training  [   620/250000]  eta: 17:00:08  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7759 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6812 (0.6842)  ibot_loss: 2.8094 (2.8090)  time: 0.163003  data: 0.000288  max mem: 4149
I20251209 21:12:18 71645 dinov2 helpers.py:102] Training  [   630/250000]  eta: 16:54:41  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7759 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6812 (0.6841)  ibot_loss: 2.8091 (2.8090)  time: 0.163621  data: 0.000311  max mem: 4149
I20251209 21:12:20 71645 dinov2 helpers.py:102] Training  [   640/250000]  eta: 16:49:25  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7696 (14.7770)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6768 (0.6840)  ibot_loss: 2.8088 (2.8090)  time: 0.163468  data: 0.000301  max mem: 4149
I20251209 21:12:21 71645 dinov2 helpers.py:102] Training  [   650/250000]  eta: 16:44:15  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7695 (14.7770)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6768 (0.6840)  ibot_loss: 2.8088 (2.8090)  time: 0.162971  data: 0.000262  max mem: 4149
I20251209 21:12:23 71645 dinov2 helpers.py:102] Training  [   660/250000]  eta: 16:39:14  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7787 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6865 (0.6841)  ibot_loss: 2.8092 (2.8090)  time: 0.162495  data: 0.000277  max mem: 4149
I20251209 21:12:25 71645 dinov2 helpers.py:102] Training  [   670/250000]  eta: 16:34:24  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7818 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6890 (0.6841)  ibot_loss: 2.8094 (2.8090)  time: 0.162746  data: 0.000305  max mem: 4149
I20251209 21:12:26 71645 dinov2 helpers.py:102] Training  [   680/250000]  eta: 16:29:42  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 2.0000 (2.0000)  total_loss: 14.7755 (14.7771)  dino_local_crops_loss: 10.0302 (10.0302)  dino_global_crops_loss: 1.2538 (1.2538)  koleo_loss: 0.6821 (0.6841)  ibot_loss: 2.8092 (2.8090)  time: 0.162995  data: 0.000311  max mem: 4149
I20251209 21:17:07 75401 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 21:17:07 75401 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 21:21:46 76733 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 21:21:46 76733 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 21:21:57 76733 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:21:58 76733 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 21:21:58 76733 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 21:21:58 76733 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 21:21:59 76733 dinov2 ssl_meta_arch.py:423] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 21:21:59 76733 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 21:21:59 76733 dinov2 param_groups.py:54] chunked fsdp
I20251209 21:21:59 76733 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 ssl_meta_arch.py:410] fusing param groups
I20251209 21:21:59 76733 dinov2 param_groups.py:64] else code branch
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 ssl_meta_arch.py:410] fusing param groups
I20251209 21:21:59 76733 dinov2 param_groups.py:64] else code branch
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:21:59 76733 dinov2 ssl_meta_arch.py:410] fusing param groups
I20251209 21:21:59 76733 dinov2 train.py:106] Schedulers ready.
W20251209 21:21:59 76733 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 21:21:59 76733 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 21:21:59 76733 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 21:21:59 76733 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 21:21:59 76733 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 21:21:59 76733 dinov2 loaders.py:141] using PyTorch data loader
I20251209 21:21:59 76733 dinov2 loaders.py:156] infinite data loader
I20251209 21:21:59 76733 dinov2 train.py:223] Starting training from iteration 0
I20251209 21:24:02 78027 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 21:24:02 78027 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 21:24:02 78027 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:24:02 78027 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 21:24:03 78027 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 21:24:03 78027 dinov2 ssl_meta_arch.py:423] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 21:24:03 78027 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 21:24:04 78027 dinov2 param_groups.py:54] chunked fsdp
I20251209 21:24:04 78027 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 ssl_meta_arch.py:410] fusing param groups
I20251209 21:24:04 78027 dinov2 param_groups.py:64] else code branch
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 ssl_meta_arch.py:410] fusing param groups
I20251209 21:24:04 78027 dinov2 param_groups.py:64] else code branch
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:24:04 78027 dinov2 ssl_meta_arch.py:410] fusing param groups
I20251209 21:24:04 78027 dinov2 train.py:106] Schedulers ready.
W20251209 21:24:04 78027 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 21:24:04 78027 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 21:24:04 78027 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 21:24:04 78027 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 21:24:04 78027 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 21:24:04 78027 dinov2 loaders.py:141] using PyTorch data loader
I20251209 21:24:04 78027 dinov2 loaders.py:156] infinite data loader
I20251209 21:24:04 78027 dinov2 train.py:223] Starting training from iteration 0
I20251209 21:25:42 79126 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 21:25:42 79126 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 21:25:42 79126 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:25:43 79126 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 21:25:43 79126 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 21:25:43 79126 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 21:25:44 79126 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 21:25:44 79126 dinov2 ssl_meta_arch.py:423] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 21:25:44 79126 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 21:25:44 79126 dinov2 param_groups.py:54] chunked fsdp
I20251209 21:25:44 79126 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 ssl_meta_arch.py:410] fusing param groups
I20251209 21:25:44 79126 dinov2 param_groups.py:64] else code branch
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 ssl_meta_arch.py:410] fusing param groups
I20251209 21:25:44 79126 dinov2 param_groups.py:64] else code branch
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:25:44 79126 dinov2 ssl_meta_arch.py:410] fusing param groups
I20251209 21:25:44 79126 dinov2 train.py:106] Schedulers ready.
W20251209 21:25:44 79126 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 21:25:44 79126 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 21:25:44 79126 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 21:25:44 79126 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 21:25:44 79126 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 21:25:44 79126 dinov2 loaders.py:141] using PyTorch data loader
I20251209 21:25:44 79126 dinov2 loaders.py:156] infinite data loader
I20251209 21:25:44 79126 dinov2 train.py:223] Starting training from iteration 0
I20251209 21:26:46 80116 dinov2 config.py:60] config_file: dinov2/configs/train/vitb_eeg.yaml
eval: 
eval_only: False
no_resume: False
opts: ['train.output_dir=/media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg']
output_dir: /media/douyl/Disk4T/douyl/IDEA_Lab/Project_BCI/dinov2/debug_output_eeg
I20251209 21:26:46 80116 dinov2 config.py:26] sqrt scaling learning rate; base: 0.0004, new: 1.767766952966369e-05
I20251209 21:26:46 80116 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:26:47 80116 dinov2 vision_transformer.py:131] using MLP layer as FFN
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:44] OPTIONS -- architecture : embed_dim: 768
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:59] OPTIONS -- DINO
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- loss_weight: 1.0
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_n_prototypes: 65536
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:64] OPTIONS -- DINO -- head_hidden_dim: 2048
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:76] OPTIONS -- DINO -- applying KOLEO regularization
W20251209 21:26:48 80116 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)

I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:89] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:97] OPTIONS -- IBOT -- loss_weight: 1.0
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:98] OPTIONS -- IBOT -- head_n_prototypes: 65536
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:99] OPTIONS -- IBOT -- head_bottleneck_dim: 256
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:100] OPTIONS -- IBOT -- head_hidden_dim: 2048
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:122] Student and Teacher are built: they are both vit_base network.
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:423] DISTRIBUTED FSDP -- preparing model for distributed training
W20251209 21:26:48 80116 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.SHARD_GRAD_OP since the world size is 1.
  warnings.warn(

I20251209 21:26:48 80116 dinov2 param_groups.py:54] chunked fsdp
I20251209 21:26:48 80116 dinov2 param_groups.py:87] cls_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] register_tokens: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mask_token: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.2, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.2, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.0.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.1.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.2.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.3.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.4.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.5.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.6.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.7.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.8.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.9.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.10.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.norm1.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.norm1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.attn.qkv.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.attn.qkv.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.attn.proj.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.attn.proj.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.ls1.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.norm2.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.norm2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.mlp.fc1.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.mlp.fc1.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.mlp.fc2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.mlp.fc2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] blocks.11.ls2.gamma: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:410] fusing param groups
I20251209 21:26:48 80116 dinov2 param_groups.py:64] else code branch
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:410] fusing param groups
I20251209 21:26:48 80116 dinov2 param_groups.py:64] else code branch
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20251209 21:26:48 80116 dinov2 ssl_meta_arch.py:410] fusing param groups
I20251209 21:26:48 80116 dinov2 train.py:106] Schedulers ready.
W20251209 21:26:48 80116 py.warnings warnings.py:112] /home/douyl/miniconda3/envs/bci/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(

I20251209 21:26:48 80116 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20251209 21:26:48 80116 dinov2 loaders.py:34] Making EEG dataset from: "./samples"
I20251209 21:26:48 80116 dinov2 loaders.py:45] # of dataset samples: 2
I20251209 21:26:48 80116 dinov2 loaders.py:72] sampler: sharded infinite
I20251209 21:26:48 80116 dinov2 loaders.py:141] using PyTorch data loader
I20251209 21:26:48 80116 dinov2 loaders.py:156] infinite data loader
I20251209 21:26:48 80116 dinov2 train.py:223] Starting training from iteration 0
